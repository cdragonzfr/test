ERROR: Could not find a version that satisfies the requirement pyinstaller-hooks-contrib>=2024.9 (from pyinstaller) (from versions: 0.0.0, 2020.4, 2020.5, 2020.6, 2020.7, 2020.8, 2020.9, 2020.10, 2020.11, 2021.1, 2021.2, 2021.3, 2021.4, 2021.5, 2022.0)
ERROR: No matching distribution found for pyinstaller-hooks-contrib>=2024.9

ERROR: Could not find a version that satisfies the requirement packaging>=22.0 (from pyinstaller) (from versions: 14.0, 14.1, 14.2, 14.3, 14.4, 14.5, 15.0, 15.1, 15.2, 15.3, 16.0, 16.1, 16.2, 16.3, 16.4, 16.5, 16.6, 16.7, 16.8, 17.0, 17.1, 18.0,
19.0, 19.1, 19.2, 20.0, 20.1, 20.2, 20.3, 20.4, 20.5, 20.6, 20.7, 20.8, 20.9, 21.0, 21.1, 21.2, 21.3)
ERROR: No matching distribution found for packaging>=22.0

        linux-vdso.so.1 (0x00007ffdfbcd7000)
        libdl.so.2 => /lib64/libdl.so.2 (0x00007f80f64db000)
        libz.so.1 => /lib64/libz.so.1 (0x00007f80f62c3000)
        libpthread.so.0 => /lib64/libpthread.so.0 (0x00007f80f60a3000)
        libc.so.6 => /lib64/libc.so.6 (0x00007f80f5ccd000)
        /lib64/ld-linux-x86-64.so.2 (0x00007f80f66df000)

openat(AT_FDCWD, "/tmp/_MEIZwgFyu/libcrypto.so.1.1", O_WRONLY|O_CREAT|O_TRUNC, 0666) = 3

usage: pyinstaller [-h] [-v] [-D] [-F] [--specpath DIR] [-n NAME]
                   [--add-data <SRC;DEST or SRC:DEST>]
                   [--add-binary <SRC;DEST or SRC:DEST>] [-p DIR]
                   [--hidden-import MODULENAME]
                   [--collect-submodules MODULENAME]
                   [--collect-data MODULENAME] [--collect-binaries MODULENAME]
                   [--collect-all MODULENAME] [--copy-metadata PACKAGENAME]
                   [--recursive-copy-metadata PACKAGENAME]
                   [--additional-hooks-dir HOOKSPATH]
                   [--runtime-hook RUNTIME_HOOKS] [--exclude-module EXCLUDES]
                   [--key KEY] [--splash IMAGE_FILE]
                   [-d {all,imports,bootloader,noarchive}]
                   [--python-option PYTHON_OPTION] [-s] [--noupx]
                   [--upx-exclude FILE] [-c] [-w]
                   [-i <FILE.ico or FILE.exe,ID or FILE.icns or "NONE">]
                   [--disable-windowed-traceback] [--version-file FILE]
                   [-m <FILE or XML>] [--no-embed-manifest] [-r RESOURCE]
                   [--uac-admin] [--uac-uiaccess] [--win-private-assemblies]
                   [--win-no-prefer-redirects]
                   [--osx-bundle-identifier BUNDLE_IDENTIFIER]
                   [--target-architecture ARCH] [--codesign-identity IDENTITY]
                   [--osx-entitlements-file FILENAME] [--runtime-tmpdir PATH]
                   [--bootloader-ignore-signals] [--distpath DIR]
                   [--workpath WORKPATH] [-y] [--upx-dir UPX_DIR] [-a]
                   [--clean] [--log-level LEVEL]
                   scriptname [scriptname ...]


Psuedo Flow

	Detect Tar GZ File in current directory (Tanium's Download Action ID Folder) and extract name
	
	Untar Tar File in current directory
	
		Make sure untar was successful, if not drop out of the script and fail
	
	Check existence of CSV needed to perform placement and permission changes
		If exists continue if it does not existâ€¦. Exit script and fail
		
	Use CSV file to iterate through each line.  Each line will have the first column that contains where the token will be placed (linux directory path), second column that contains the name of the token, third column containing the owner permission that the token will have on the operation system, fourth column containing the file system permissions for owner:group:everyone for that token. 
	
	Each iteration will perform the chown on the token based on the third column first, then perform chmod on the token based on the fourth column file system permissions.  Lastly the token will then be moved out of the current directory to the linux directory path in the first column.   Lastly, it will need to perform a check to verify that the token is in the location it was moved to and show the owner and permissions it has.  
	
All of these operations will be logged and will be placed in the /tmp folder.  Pull back with Tanium the log file for review 



Feb 06 12:51:54 hostname firewalld[42434]: WARNING: COMMAND_FAILED: '/usr/sbin/iptables -w10 -t nat -D PREROUTING -m addrtype --dst-type LOCAL -j DOCKER' failed: iptables: Bad rule (does a matching rule exist in that chain?).
Feb 06 12:51:54 hostname firewalld[42434]: WARNING: COMMAND_FAILED: '/usr/sbin/iptables -w10 -t nat -D OUTPUT -m addrtype --dst-type LOCAL ! --dst 127.0.0.0/8 -j DOCKER' failed: iptables: Bad rule (does a matching rule exist in that chain?>
Feb 06 12:51:54 hostname firewalld[42434]: WARNING: COMMAND_FAILED: '/usr/sbin/iptables -w10 -t nat -D OUTPUT -m addrtype --dst-type LOCAL -j DOCKER' failed: iptables: Bad rule (does a matching rule exist in that chain?).
Feb 06 12:51:54 hostname firewalld[42434]: WARNING: COMMAND_FAILED: '/usr/sbin/iptables -w10 -t nat -D PREROUTING' failed: iptables: Bad rule (does a matching rule exist in that chain?).
Feb 06 12:51:54 hostname firewalld[42434]: WARNING: COMMAND_FAILED: '/usr/sbin/iptables -w10 -t nat -D OUTPUT' failed: iptables: Bad rule (does a matching rule exist in that chain?).
Feb 06 12:51:54 hostname firewalld[42434]: WARNING: COMMAND_FAILED: '/usr/sbin/iptables -w10 -t filter -F DOCKER-ISOLATION' failed: iptables: No chain/target/match by that name.
Feb 06 12:51:54 hostname firewalld[42434]: WARNING: COMMAND_FAILED: '/usr/sbin/iptables -w10 -t filter -X DOCKER-ISOLATION' failed: iptables: No chain/target/match by that name.
Feb 06 12:51:54 hostname dockerd[228419]: time="2025-02-06T12:51:54.713457722-05:00" level=info msg="Default bridge (docker0) is assigned with an IP address 172.17.0.0/16. Daemon option --bip can be used to set a preferred IP address"
Feb 06 12:51:54 hostname firewalld[42434]: ERROR: 'python-nftables' failed: internal:0:0-0: Error: Could not process rule: No such file or directory

                                                 internal:0:0-0: Error: Could not process rule: No such file or directory

                                                 internal:0:0-0: Error: Could not process rule: No such file or directory

                                                 internal:0:0-0: Error: Could not process rule: No such file or directory

                                                 internal:0:0-0: Error: Could not process rule: No such file or directory

                                                 internal:0:0-0: Error: Could not process rule: No such file or directory

                                                 internal:0:0-0: Error: Could not process rule: No such file or directory

                                                 internal:0:0-0: Error: Could not process rule: No such file or directory


                                                 JSON blob:
                                                 {"nftables": [{"metainfo": {"json_schema_version": 1}}, {"insert": {"rule": {"family": "inet", "table": "firewalld", "chain": "filter_INPUT_ZONES", "expr": [{"match": {"left": {"meta": {"key": "ii>
Feb 06 12:51:54 hostname firewalld[42434]: ERROR: COMMAND_FAILED: 'python-nftables' failed: internal:0:0-0: Error: Could not process rule: No such file or directory

                                                 internal:0:0-0: Error: Could not process rule: No such file or directory

                                                 internal:0:0-0: Error: Could not process rule: No such file or directory

                                                 internal:0:0-0: Error: Could not process rule: No such file or directory

                                                 internal:0:0-0: Error: Could not process rule: No such file or directory

                                                 internal:0:0-0: Error: Could not process rule: No such file or directory

                                                 internal:0:0-0: Error: Could not process rule: No such file or directory

                                                 internal:0:0-0: Error: Could not process rule: No such file or directory


                                                 JSON blob:
                                                 {"nftables": [{"metainfo": {"json_schema_version": 1}}, {"insert": {"rule": {"family": "inet", "table": "firewalld", "chain": "filter_INPUT_ZONES", "expr": [{"match": {"left": {"meta": {"key": "ii>
Feb 06 12:51:54 hostname dockerd[228419]: time="2025-02-06T12:51:54.791760799-05:00" level=info msg="stopping event stream following graceful shutdown" error="<nil>" module=libcontainerd namespace=moby
Feb 06 12:51:54 hostname dockerd[228419]: time="2025-02-06T12:51:54.792104743-05:00" level=info msg="stopping event stream following graceful shutdown" error="context canceled" module=libcontainerd namespace=plugins.moby
Feb 06 12:51:54 hostname dockerd[228419]: failed to start daemon: Error initializing network controller: error creating default "bridge" network: Failed to program NAT chain: COMMAND_FAILED: 'python-nftables' failed: internal:0:0-0: Error:>
Feb 06 12:51:54 hostname dockerd[228419]: internal:0:0-0: Error: Could not process rule: No such file or directory
Feb 06 12:51:54 hostname dockerd[228419]: JSON blob:
Feb 06 12:51:54 hostname dockerd[228419]: {"nftables": [{"metainfo": {"json_schema_version": 1}}, {"insert": {"rule": {"family": "inet", "table": "firewalld", "chain": "filter_INPUT_ZONES", "expr": [{"match": {"left": {"meta": {



AUDIT_2FDF7345-BC49-4140-BB1F-196D7EDBB87D_0_133836678750390000.sqlaudit
AUDIT_2FDF7345-BC49-4140-BB1F-196D7EDBB87D_0_133836691443240000.sqlaudit
AUDIT_2FDF7345-BC49-4140-BB1F-196D7EDBB87D_0_133836702058330000.sqlaudit
AUDIT_2FDF7345-BC49-4140-BB1F-196D7EDBB87D_0_133836712261190000.sqlaudit
AUDIT_2FDF7345-BC49-4140-BB1F-196D7EDBB87D_0_133836725105720000.sqlaudit
AUDIT_2FDF7345-BC49-4140-BB1F-196D7EDBB87D_0_133836736850170000.sqlaudit
AUDIT_2FDF7345-BC49-4140-BB1F-196D7EDBB87D_0_133836746723070000.sqlaudit
AUDIT_2FDF7345-BC49-4140-BB1F-196D7EDBB87D_0_133836758767370000.sqlaudit
AUDIT_2FDF7345-BC49-4140-BB1F-196D7EDBB87D_0_133836767600630000.sqlaudit
AUDIT_2FDF7345-BC49-4140-BB1F-196D7EDBB87D_0_133836780643770000.sqlaudit

#!/bin/bash

# Define the path to the master list file
masterListPath="master_list.csv"

# Define the path for the output CSV file
outputCsvPath="/tmp/output.csv"

# Check if the output CSV file already exists and remove it if it does
if [ -f "$outputCsvPath" ]; then
    rm "$outputCsvPath"
fi

# Get the current system's hostname and strip the domain if present, then convert to uppercase
currentHostname=$(hostname | cut -d. -f1 | tr '[:lower:]' '[:upper:]')

# Read the master list file and filter the records
# Assuming the CSV file has headers with 'Hostname' and 'DirectoryPath'
filteredRecords=$(awk -F, -v host="$currentHostname" 'toupper($1) == host {print $0}' "$masterListPath")

# Initialize the output CSV file and write the headers
echo "Hostname,FilePath,Presence" > "$outputCsvPath"

# Set IFS to comma and newline
IFS=$',\n'

# Iterate through each filtered record
echo "$filteredRecords" | while read -r line; do
    # Read hostname and directoryPath from the line
    hostname=$(echo "$line" | cut -d, -f1)
    directoryPath=$(echo "$line" | cut -d, -f2)

    # Remove potential carriage return characters
    directoryPath=$(echo "$directoryPath" | tr -d '\r')

    presence=0

    # Check if the file exists (using -f for files)
    if [ -f "$directoryPath" ]; then
        presence=1
    fi

    # Write the data to the output CSV file
    echo "$currentHostname,$directoryPath,$presence" >> "$outputCsvPath"
done

# Reset IFS to default
unset IFS

# Optional: Display a message when done
echo "Output written to $outputCsvPath"


sed 's/),(/);\nINSERT INTO/g' dumpfile.sql > formatted_dumpfile.sql

iconv -f ASCII -t UTF-8 formatted_dumpfile.sql -o dumpfile_utf8.sql

while (<$SQLFILE>) {
while (my $line = <$SQLFILE>) {
    $line =~ s/\n/ /g;  # Replace newlines with spaces
    $line =~ s/  +/ /g; # Remove excessive spaces

perl -d mysql2sqlite.pl formatted_dumpfile.sql


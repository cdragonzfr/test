ERROR: Could not find a version that satisfies the requirement pyinstaller-hooks-contrib>=2024.9 (from pyinstaller) (from versions: 0.0.0, 2020.4, 2020.5, 2020.6, 2020.7, 2020.8, 2020.9, 2020.10, 2020.11, 2021.1, 2021.2, 2021.3, 2021.4, 2021.5, 2022.0)
ERROR: No matching distribution found for pyinstaller-hooks-contrib>=2024.9

ERROR: Could not find a version that satisfies the requirement packaging>=22.0 (from pyinstaller) (from versions: 14.0, 14.1, 14.2, 14.3, 14.4, 14.5, 15.0, 15.1, 15.2, 15.3, 16.0, 16.1, 16.2, 16.3, 16.4, 16.5, 16.6, 16.7, 16.8, 17.0, 17.1, 18.0,
19.0, 19.1, 19.2, 20.0, 20.1, 20.2, 20.3, 20.4, 20.5, 20.6, 20.7, 20.8, 20.9, 21.0, 21.1, 21.2, 21.3)
ERROR: No matching distribution found for packaging>=22.0

        linux-vdso.so.1 (0x00007ffdfbcd7000)
        libdl.so.2 => /lib64/libdl.so.2 (0x00007f80f64db000)
        libz.so.1 => /lib64/libz.so.1 (0x00007f80f62c3000)
        libpthread.so.0 => /lib64/libpthread.so.0 (0x00007f80f60a3000)
        libc.so.6 => /lib64/libc.so.6 (0x00007f80f5ccd000)
        /lib64/ld-linux-x86-64.so.2 (0x00007f80f66df000)

openat(AT_FDCWD, "/tmp/_MEIZwgFyu/libcrypto.so.1.1", O_WRONLY|O_CREAT|O_TRUNC, 0666) = 3

usage: pyinstaller [-h] [-v] [-D] [-F] [--specpath DIR] [-n NAME]
                   [--add-data <SRC;DEST or SRC:DEST>]
                   [--add-binary <SRC;DEST or SRC:DEST>] [-p DIR]
                   [--hidden-import MODULENAME]
                   [--collect-submodules MODULENAME]
                   [--collect-data MODULENAME] [--collect-binaries MODULENAME]
                   [--collect-all MODULENAME] [--copy-metadata PACKAGENAME]
                   [--recursive-copy-metadata PACKAGENAME]
                   [--additional-hooks-dir HOOKSPATH]
                   [--runtime-hook RUNTIME_HOOKS] [--exclude-module EXCLUDES]
                   [--key KEY] [--splash IMAGE_FILE]
                   [-d {all,imports,bootloader,noarchive}]
                   [--python-option PYTHON_OPTION] [-s] [--noupx]
                   [--upx-exclude FILE] [-c] [-w]
                   [-i <FILE.ico or FILE.exe,ID or FILE.icns or "NONE">]
                   [--disable-windowed-traceback] [--version-file FILE]
                   [-m <FILE or XML>] [--no-embed-manifest] [-r RESOURCE]
                   [--uac-admin] [--uac-uiaccess] [--win-private-assemblies]
                   [--win-no-prefer-redirects]
                   [--osx-bundle-identifier BUNDLE_IDENTIFIER]
                   [--target-architecture ARCH] [--codesign-identity IDENTITY]
                   [--osx-entitlements-file FILENAME] [--runtime-tmpdir PATH]
                   [--bootloader-ignore-signals] [--distpath DIR]
                   [--workpath WORKPATH] [-y] [--upx-dir UPX_DIR] [-a]
                   [--clean] [--log-level LEVEL]
                   scriptname [scriptname ...]


Psuedo Flow

	Detect Tar GZ File in current directory (Tanium's Download Action ID Folder) and extract name
	
	Untar Tar File in current directory
	
		Make sure untar was successful, if not drop out of the script and fail
	
	Check existence of CSV needed to perform placement and permission changes
		If exists continue if it does not existâ€¦. Exit script and fail
		
	Use CSV file to iterate through each line.  Each line will have the first column that contains where the token will be placed (linux directory path), second column that contains the name of the token, third column containing the owner permission that the token will have on the operation system, fourth column containing the file system permissions for owner:group:everyone for that token. 
	
	Each iteration will perform the chown on the token based on the third column first, then perform chmod on the token based on the fourth column file system permissions.  Lastly the token will then be moved out of the current directory to the linux directory path in the first column.   Lastly, it will need to perform a check to verify that the token is in the location it was moved to and show the owner and permissions it has.  
	
All of these operations will be logged and will be placed in the /tmp folder.  Pull back with Tanium the log file for review 



Feb 06 12:51:54 hostname firewalld[42434]: WARNING: COMMAND_FAILED: '/usr/sbin/iptables -w10 -t nat -D PREROUTING -m addrtype --dst-type LOCAL -j DOCKER' failed: iptables: Bad rule (does a matching rule exist in that chain?).
Feb 06 12:51:54 hostname firewalld[42434]: WARNING: COMMAND_FAILED: '/usr/sbin/iptables -w10 -t nat -D OUTPUT -m addrtype --dst-type LOCAL ! --dst 127.0.0.0/8 -j DOCKER' failed: iptables: Bad rule (does a matching rule exist in that chain?>
Feb 06 12:51:54 hostname firewalld[42434]: WARNING: COMMAND_FAILED: '/usr/sbin/iptables -w10 -t nat -D OUTPUT -m addrtype --dst-type LOCAL -j DOCKER' failed: iptables: Bad rule (does a matching rule exist in that chain?).
Feb 06 12:51:54 hostname firewalld[42434]: WARNING: COMMAND_FAILED: '/usr/sbin/iptables -w10 -t nat -D PREROUTING' failed: iptables: Bad rule (does a matching rule exist in that chain?).
Feb 06 12:51:54 hostname firewalld[42434]: WARNING: COMMAND_FAILED: '/usr/sbin/iptables -w10 -t nat -D OUTPUT' failed: iptables: Bad rule (does a matching rule exist in that chain?).
Feb 06 12:51:54 hostname firewalld[42434]: WARNING: COMMAND_FAILED: '/usr/sbin/iptables -w10 -t filter -F DOCKER-ISOLATION' failed: iptables: No chain/target/match by that name.
Feb 06 12:51:54 hostname firewalld[42434]: WARNING: COMMAND_FAILED: '/usr/sbin/iptables -w10 -t filter -X DOCKER-ISOLATION' failed: iptables: No chain/target/match by that name.
Feb 06 12:51:54 hostname dockerd[228419]: time="2025-02-06T12:51:54.713457722-05:00" level=info msg="Default bridge (docker0) is assigned with an IP address 172.17.0.0/16. Daemon option --bip can be used to set a preferred IP address"
Feb 06 12:51:54 hostname firewalld[42434]: ERROR: 'python-nftables' failed: internal:0:0-0: Error: Could not process rule: No such file or directory

                                                 internal:0:0-0: Error: Could not process rule: No such file or directory

                                                 internal:0:0-0: Error: Could not process rule: No such file or directory

                                                 internal:0:0-0: Error: Could not process rule: No such file or directory

                                                 internal:0:0-0: Error: Could not process rule: No such file or directory

                                                 internal:0:0-0: Error: Could not process rule: No such file or directory

                                                 internal:0:0-0: Error: Could not process rule: No such file or directory

                                                 internal:0:0-0: Error: Could not process rule: No such file or directory


                                                 JSON blob:
                                                 {"nftables": [{"metainfo": {"json_schema_version": 1}}, {"insert": {"rule": {"family": "inet", "table": "firewalld", "chain": "filter_INPUT_ZONES", "expr": [{"match": {"left": {"meta": {"key": "ii>
Feb 06 12:51:54 hostname firewalld[42434]: ERROR: COMMAND_FAILED: 'python-nftables' failed: internal:0:0-0: Error: Could not process rule: No such file or directory

                                                 internal:0:0-0: Error: Could not process rule: No such file or directory

                                                 internal:0:0-0: Error: Could not process rule: No such file or directory

                                                 internal:0:0-0: Error: Could not process rule: No such file or directory

                                                 internal:0:0-0: Error: Could not process rule: No such file or directory

                                                 internal:0:0-0: Error: Could not process rule: No such file or directory

                                                 internal:0:0-0: Error: Could not process rule: No such file or directory

                                                 internal:0:0-0: Error: Could not process rule: No such file or directory


                                                 JSON blob:
                                                 {"nftables": [{"metainfo": {"json_schema_version": 1}}, {"insert": {"rule": {"family": "inet", "table": "firewalld", "chain": "filter_INPUT_ZONES", "expr": [{"match": {"left": {"meta": {"key": "ii>
Feb 06 12:51:54 hostname dockerd[228419]: time="2025-02-06T12:51:54.791760799-05:00" level=info msg="stopping event stream following graceful shutdown" error="<nil>" module=libcontainerd namespace=moby
Feb 06 12:51:54 hostname dockerd[228419]: time="2025-02-06T12:51:54.792104743-05:00" level=info msg="stopping event stream following graceful shutdown" error="context canceled" module=libcontainerd namespace=plugins.moby
Feb 06 12:51:54 hostname dockerd[228419]: failed to start daemon: Error initializing network controller: error creating default "bridge" network: Failed to program NAT chain: COMMAND_FAILED: 'python-nftables' failed: internal:0:0-0: Error:>
Feb 06 12:51:54 hostname dockerd[228419]: internal:0:0-0: Error: Could not process rule: No such file or directory
Feb 06 12:51:54 hostname dockerd[228419]: JSON blob:
Feb 06 12:51:54 hostname dockerd[228419]: {"nftables": [{"metainfo": {"json_schema_version": 1}}, {"insert": {"rule": {"family": "inet", "table": "firewalld", "chain": "filter_INPUT_ZONES", "expr": [{"match": {"left": {"meta": {



AUDIT_2FDF7345-BC49-4140-BB1F-196D7EDBB87D_0_133836678750390000.sqlaudit
AUDIT_2FDF7345-BC49-4140-BB1F-196D7EDBB87D_0_133836691443240000.sqlaudit
AUDIT_2FDF7345-BC49-4140-BB1F-196D7EDBB87D_0_133836702058330000.sqlaudit
AUDIT_2FDF7345-BC49-4140-BB1F-196D7EDBB87D_0_133836712261190000.sqlaudit
AUDIT_2FDF7345-BC49-4140-BB1F-196D7EDBB87D_0_133836725105720000.sqlaudit
AUDIT_2FDF7345-BC49-4140-BB1F-196D7EDBB87D_0_133836736850170000.sqlaudit
AUDIT_2FDF7345-BC49-4140-BB1F-196D7EDBB87D_0_133836746723070000.sqlaudit
AUDIT_2FDF7345-BC49-4140-BB1F-196D7EDBB87D_0_133836758767370000.sqlaudit
AUDIT_2FDF7345-BC49-4140-BB1F-196D7EDBB87D_0_133836767600630000.sqlaudit
AUDIT_2FDF7345-BC49-4140-BB1F-196D7EDBB87D_0_133836780643770000.sqlaudit

#!/bin/bash

# Define the path to the master list file
masterListPath="master_list.csv"

# Define the path for the output CSV file
outputCsvPath="/tmp/output.csv"

# Check if the output CSV file already exists and remove it if it does
if [ -f "$outputCsvPath" ]; then
    rm "$outputCsvPath"
fi

# Get the current system's hostname and strip the domain if present, then convert to uppercase
currentHostname=$(hostname | cut -d. -f1 | tr '[:lower:]' '[:upper:]')

# Read the master list file and filter the records
# Assuming the CSV file has headers with 'Hostname' and 'DirectoryPath'
filteredRecords=$(awk -F, -v host="$currentHostname" 'toupper($1) == host {print $0}' "$masterListPath")

# Initialize the output CSV file and write the headers
echo "Hostname,FilePath,Presence" > "$outputCsvPath"

# Set IFS to comma and newline
IFS=$',\n'

# Iterate through each filtered record
echo "$filteredRecords" | while read -r line; do
    # Read hostname and directoryPath from the line
    hostname=$(echo "$line" | cut -d, -f1)
    directoryPath=$(echo "$line" | cut -d, -f2)

    # Remove potential carriage return characters
    directoryPath=$(echo "$directoryPath" | tr -d '\r')

    presence=0

    # Check if the file exists (using -f for files)
    if [ -f "$directoryPath" ]; then
        presence=1
    fi

    # Write the data to the output CSV file
    echo "$currentHostname,$directoryPath,$presence" >> "$outputCsvPath"
done

# Reset IFS to default
unset IFS

# Optional: Display a message when done
echo "Output written to $outputCsvPath"


sed 's/),(/);\nINSERT INTO/g' dumpfile.sql > formatted_dumpfile.sql

iconv -f ASCII -t UTF-8 formatted_dumpfile.sql -o dumpfile_utf8.sql

while (<$SQLFILE>) {
while (my $line = <$SQLFILE>) {
    $line =~ s/\n/ /g;  # Replace newlines with spaces
    $line =~ s/  +/ /g; # Remove excessive spaces

perl -d mysql2sqlite.pl formatted_dumpfile.sql

sudo docker stop mysql-container
sudo docker rm mysql-container
sudo docker run -d --name mysql-container \
  -e MYSQL_ROOT_PASSWORD=yourpassword \
  -p 3306:3306 \
  -v mysql_data:/var/lib/mysql \
  --entrypoint mysqld \
  mysql:8 --local-infile=1


2025-03-06 19:46:41,332 - test_invicti_activity - [ERROR] - [test] HTTPError reason=HTTP Error HTTPSConnectionPool(host='redacted', port=443): Max retries exceeded with url: /api/1.0/auditlogs/list?page=1&pageSize=20&startDate=03/01/2025%2000:00:00&endDate=03/05/2025%2000:00:00 (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1143)'))) when sending request to url=https://redacted/api/1.0/auditlogs/list?page=1&pageSize=20&startDate=03/01/2025 00:00:00&endDate=03/05/2025 00:00:00 method=GET
Traceback (most recent call last):
  File "/opt/splunk/etc/apps/TA-redacted-invicti-activity/bin/ta_redacted_invicti_activity/aob_py3/urllib3/connectionpool.py", line 715, in urlopen
    httplib_response = self._make_request(
  File "/opt/splunk/etc/apps/TA-redacted-invicti-activity/bin/ta_redacted_invicti_activity/aob_py3/urllib3/connectionpool.py", line 404, in _make_request
    self._validate_conn(conn)
  File "/opt/splunk/etc/apps/TA-redacted-invicti-activity/bin/ta_redacted_invicti_activity/aob_py3/urllib3/connectionpool.py", line 1060, in _validate_conn
    conn.connect()
  File "/opt/splunk/etc/apps/TA-redacted-invicti-activity/bin/ta_redacted_invicti_activity/aob_py3/urllib3/connection.py", line 419, in connect
    self.sock = ssl_wrap_socket(
  File "/opt/splunk/etc/apps/TA-redacted-invicti-activity/bin/ta_redacted_invicti_activity/aob_py3/urllib3/util/ssl_.py", line 449, in ssl_wrap_socket
    ssl_sock = _ssl_wrap_socket_impl(
  File "/opt/splunk/etc/apps/TA-redacted-invicti-activity/bin/ta_redacted_invicti_activity/aob_py3/urllib3/util/ssl_.py", line 493, in _ssl_wrap_socket_impl
    return ssl_context.wrap_socket(sock, server_hostname=server_hostname)
  File "/opt/splunk/lib/python3.9/ssl.py", line 506, in wrap_socket
    return self.sslsocket_class._create(
  File "/opt/splunk/lib/python3.9/ssl.py", line 1049, in _create
    self.do_handshake()
  File "/opt/splunk/lib/python3.9/ssl.py", line 1318, in do_handshake
    self._sslobj.do_handshake()
ssl.SSLCertVerificationError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1143)
During handling of the above exception, another exception occurred:
Traceback (most recent call last):
  File "/opt/splunk/etc/apps/TA-redacted-invicti-activity/bin/ta_redacted_invicti_activity/aob_py3/requests/adapters.py", line 486, in send
    resp = conn.urlopen(
  File "/opt/splunk/etc/apps/TA-redacted-invicti-activity/bin/ta_redacted_invicti_activity/aob_py3/urllib3/connectionpool.py", line 801, in urlopen
    retries = retries.increment(
  File "/opt/splunk/etc/apps/TA-redacted-invicti-activity/bin/ta_redacted_invicti_activity/aob_py3/urllib3/util/retry.py", line 594, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='redacted', port=443): Max retries exceeded with url: /api/1.0/auditlogs/list?page=1&pageSize=20&startDate=03/01/2025%2000:00:00&endDate=03/05/2025%2000:00:00 (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1143)')))
During handling of the above exception, another exception occurred:
Traceback (most recent call last):
  File "/opt/splunk/etc/apps/TA-redacted-invicti-activity/bin/ta_redacted_invicti_activity/aob_py3/cloudconnectlib/core/http.py", line 229, in _retry_send_request_if_needed
    resp = self._send_internal(
  File "/opt/splunk/etc/apps/TA-redacted-invicti-activity/bin/ta_redacted_invicti_activity/aob_py3/cloudconnectlib/core/http.py", line 213, in _send_internal
    return self._connection.request(
  File "/opt/splunk/etc/apps/TA-redacted-invicti-activity/bin/ta_redacted_invicti_activity/aob_py3/requests/sessions.py", line 589, in request
    resp = self.send(prep, **send_kwargs)
  File "/opt/splunk/etc/apps/TA-redacted-invicti-activity/bin/ta_redacted_invicti_activity/aob_py3/requests/sessions.py", line 703, in send
    r = adapter.send(request, **kwargs)
  File "/opt/splunk/etc/apps/TA-redacted-invicti-activity/bin/ta_redacted_invicti_activity/aob_py3/requests/adapters.py", line 517, in send
    raise SSLError(e, request=request)
requests.exceptions.SSLError: HTTPSConnectionPool(host='redacted', port=443): Max retries exceeded with url: /api/1.0/auditlogs/list?page=1&pageSize=20&startDate=03/01/2025%2000:00:00&endDate=03/05/2025%2000:00:00 (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1143)')))
The above exception was the direct cause of the following exception:
Traceback (most recent call last):
  File "/opt/splunk/etc/apps/TA-redacted-invicti-activity/bin/ta_redacted_invicti_activity/aob_py3/cloudconnectlib/core/engine.py", line 308, in _send_request
    response = self._client.send(request)
  File "/opt/splunk/etc/apps/TA-redacted-invicti-activity/bin/ta_redacted_invicti_activity/aob_py3/cloudconnectlib/core/http.py", line 295, in send
    return self._retry_send_request_if_needed(
  File "/opt/splunk/etc/apps/TA-redacted-invicti-activity/bin/ta_redacted_invicti_activity/aob_py3/cloudconnectlib/core/http.py", line 243, in _retry_send_request_if_needed
    raise HTTPError(f"HTTP Error {err}") from err
cloudconnectlib.core.exceptions.HTTPError: HTTP Error HTTPSConnectionPool(host='redacted', port=443): Max retries exceeded with url: /api/1.0/auditlogs/list?page=1&pageSize=20&startDate=03/01/2025%2000:00:00&endDate=03/05/2025%2000:00:00 (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1143)')))



import os
import sys
import time
import datetime

'''
    IMPORTANT
    Edit only the validate_input and collect_events functions.
    Do not edit any other part in this file.
    This file is generated only once when creating the modular input.
'''

# For advanced users, if you want to create single instance mod input, uncomment this method.
#def use_single_instance_mode():
#    return True


def validate_input(helper, definition):
    """Implement your own validation logic to validate the input stanza configurations"""
    # This example accesses the modular input variable
    userid = definition.parameters.get('userid', None)
    api_key = definition.parameters.get('api_key', None)
    pass

def collect_events(helper, ew):
    # Implement your data collection logic here

    # The following examples get the arguments of this input.
    # Note, for single instance mod input, args will be returned as a dict.
    # For multi instance mod input, args will be returned as a single value.
    #opt_userid = helper.get_arg('userid')
    #opt_api_key = helper.get_arg('api_key')
    # In single instance mode, to get arguments of a particular input, use
    #opt_userid = helper.get_arg('userid', stanza_name)
    #opt_api_key = helper.get_arg('api_key', stanza_name)
    
    opt_userid = 'xyz'
    opt_api_key = 'xyz='
    
    
    # get input type
    helper.get_input_type()

    # The following examples get input stanzas.
    # get all detailed input stanzas
    helper.get_input_stanza()
    # get specific input stanza with stanza name
    helper.get_input_stanza(stanza_name)
    # get all stanza names
    helper.get_input_stanza_names()

    # The following examples get options from setup page configuration.
    # get the loglevel from the setup page
    loglevel = helper.get_log_level()
    # get proxy setting configuration
    proxy_settings = helper.get_proxy()
    # get account credentials as dictionary
    account = helper.get_user_credential_by_username("username")
    account = helper.get_user_credential_by_id("account id")
    # get global variable configuration
    global_userdefined_global_var = helper.get_global_setting("userdefined_global_var")

    # The following examples show usage of logging related helper functions.
    # write to the log for this modular input using configured global log level or INFO as default
    helper.log("log message")
    # write to the log using specified log level
    helper.log_debug("log message")
    helper.log_info("log message")
    helper.log_warning("log message")
    helper.log_error("log message")
    helper.log_critical("log message")
    # set the log level for this modular input
    # (log_level can be "debug", "info", "warning", "error" or "critical", case insensitive)
    helper.set_log_level(log_level)

    # The following examples send rest requests to some endpoint.
    response = helper.send_http_request(url, method, parameters=None, payload=None,
                                        headers=None, cookies=None, verify=True, cert=None,
                                        timeout=None, use_proxy=True)
    # get the response headers
    r_headers = response.headers
    # get the response body as text
    r_text = response.text
    # get response body as json. If the body text is not a json string, raise a ValueError
    r_json = response.json()
    # get response cookies
    r_cookies = response.cookies
    # get redirect history
    historical_responses = response.history
    # get response status code
    r_status = response.status_code
    # check the response status, if the status is not sucessful, raise requests.HTTPError
    response.raise_for_status()

    # The following examples show usage of check pointing related helper functions.
    # save checkpoint
    helper.save_check_point(key, state)
    # delete checkpoint
    helper.delete_check_point(key)
    # get checkpoint
    state = helper.get_check_point(key)

    # To create a splunk event
    helper.new_event(data, time=None, host=None, index=None, source=None, sourcetype=None, done=True, unbroken=True)
    
Traceback (most recent call last):
  File "/opt/splunk/etc/apps/TA-redacted-invicti_activity/bin/ta_redacted_invicti_activity/aob_py3/modinput_wrapper/base_modinput.py", line 128, in stream_events
    self.collect_events(ew)
  File "/opt/splunk/etc/apps/TA-redacted-invicti_activity/bin/invictitest_1741356581_605.py", line 68, in collect_events
    input_module.collect_events(self, ew)
  File "/opt/splunk/etc/apps/TA-redacted-invicti_activity/bin/input_module_invictitest_1741356581_605.py", line 70, in collect_events
    helper.log("log message")
  File "/opt/splunk/etc/apps/TA-redacted-invicti_activity/bin/ta_redacted_invicti_activity/aob_py3/modinput_wrapper/base_modinput.py", line 281, in log
    self.logger.log(level=self.log_level, msg=msg)
  File "/opt/splunk/lib/python3.9/logging/__init__.py", line 1528, in log
    raise TypeError("level must be an integer")
TypeError: level must be an integer

ERROR level must be an integer - Traceback (most recent call last):   File "/opt/splunk/etc/apps/TA-redacted-invicti_activity/bin/ta_redacted_invicti_activity/aob_py3/modinput_wrapper/base_modinput.py", line 128, in stream_events     self.collect_events(ew)   File "/opt/splunk/etc/apps/TA-redacted-invicti_activity/bin/invictitest_1741356581_605.py", line 68, in collect_events     input_module.collect_events(self, ew)   File "/opt/splunk/etc/apps/TA-redacted-invicti_activity/bin/input_module_invictitest_1741356581_605.py", line 70, in collect_events     helper.log("log message")   File "/opt/splunk/etc/apps/TA-redacted-invicti_activity/bin/ta_redacted_invicti_activity/aob_py3/modinput_wrapper/base_modinput.py", line 281, in log     self.logger.log(level=self.log_level, msg=msg)   File "/opt/splunk/lib/python3.9/logging/__init__.py", line 1528, in log     raise TypeError("level must be an integer") TypeError: level must be an integer  During handling of the above exception, another exception occurred:  Traceback (most recent call last):   File "/opt/splunk/etc/apps/TA-redacted-invicti_activity/bin/ta_redacted_invicti_activity/aob_py3/splunklib/modularinput/script.py", line 67, in run_script     self.stream_events(self._input_definition, event_writer)   File "/opt/splunk/etc/apps/TA-redacted-invicti_activity/bin/ta_redacted_invicti_activity/aob_py3/modinput_wrapper/base_modinput.py", line 134, in stream_events     raise RuntimeError(str(e)) RuntimeError: level must be an integer 
